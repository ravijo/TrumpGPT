{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "2WAdx1G4-J1n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_path_starts_with(starts_with=[\"lora\", \"distilgpt\", \"sample_data\"]):\n",
        "    for entry in os.listdir():\n",
        "        if os.path.isdir(entry) and any(entry.startswith(prefix) for prefix in starts_with):\n",
        "            shutil.rmtree(entry, ignore_errors=True)\n",
        "            print(f\"Deleted folder: {entry}\")"
      ],
      "metadata": {
        "id": "PV_jTopr-GnA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_path_starts_with()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5DAtXrt-LwO",
        "outputId": "886a0557-0514-400f-923d-98538a6c169f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted folder: sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MOsHUjgdIrIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed57b66c-3ba8-4a12-9cd8-1039d7f1060b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aB_rajDO0gYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "48e26e8f-8e48-4bb0-ad4b-cb77c7be51f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.51.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_n9OWV3l-Q"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "ovYPquUfKxGX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
        "dataset = load_dataset('jonaskoenig/trump_administration_statement', data_files=data_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "30d9d666498f40038943b0eb69f38819",
            "dc4ff60656b540c282f9803dfbac7d31",
            "835ad497ae194bc09346651b89fb88a6",
            "8b34c1a161194e289d83366945c99e28",
            "54c3022cfa364eb585d97f187fed9872",
            "b40ccedd2bf64f128356d17f5ec7c557",
            "6dc456e6761f41e5820c74e2c3236231",
            "6ef080f8f05e4237965f37072e6c76eb",
            "459dac6bebfb41b6a8f7f779189712c8",
            "bc69112b52dd427e97441d11374aca45",
            "e4dee6e8d51b47f78fb0a7a2e72610d3",
            "92b9393af1d041929ab074ab02243d18",
            "1093509ff071489a930a0512338ac35b",
            "17f4a0aa43a447859f07477d9ddb1825",
            "8fca06c0795146f28e09dcb6cd19c9dd",
            "480613ea56f44088bf7fe25a971be2c1",
            "2a4ffb2acb03431eb9c6810d54b7647f",
            "a805cf850c4c4c98a695ad497156f89d",
            "e4f1049534e9431a8bea063997caa1ec",
            "852b859dcdb943149afb86483f88fcb4",
            "9f2399dffa8a4e42a23a17e3584367bf",
            "60870d55412649369c1ca59ae47ad79c",
            "c91a2bd7ebf943dbaa682a5bc30dfbf6",
            "42716bdbf4624ff5a97f58bfdca08daf",
            "8a7f4c7472d949df8578bd79846665fe",
            "8d1f28165ddc472f8592c3a747af9287",
            "bf768693921f49bbb7618ee5bb1b8692",
            "98a4eaa1df3342509ffdcbb9351e0829",
            "0f0fbe81609c481f80f12f3a0d310746",
            "f2dca5f7f0014b9d9726e303b01f5c9e",
            "14d0dfd4bdd841528da627f032a15695",
            "b2f47559c6144966a35a2745d87611f9",
            "e20785fe20884974849f64d840b0abc0",
            "af81b3970d2e4a2c8dec6bbab10d1283",
            "e9ac646ce2734d0094da599bd917e132",
            "701ca17f34034455882cbc47c455bb96",
            "4409fd76f19c4b4a9fff66cdca0036cf",
            "4c88059a3c544760b451845612ca3ac3",
            "e5503712a82740e99c24d3c0d427316f",
            "d05b526dfbe44462bc53000d7a26d1c5",
            "192aefae6b414e78adf60b5d42a7a1da",
            "c455c45380974cb8afd9da5e28b6f42d",
            "370483a1451f4b43b356b5026ecbef51",
            "2447002512494803a3bba3bf0ebcb0d7",
            "f4f30a9dbc0b4b62a137d4eb742c99af",
            "c6c23eda50d44fc3912842040d7717e1",
            "667d44895c7f49b2b878233459752548",
            "7529aaa419614fca95b4a10e45dac339",
            "2544c96f343c4d51bfa1ece2e5c9fa9d",
            "3b89f339bc214fd8a97a76eafd861e83",
            "776e564cb2fb4e9883c777c6b4eef234",
            "9194d58cdf7d4837abfd4a0a437cd6a0",
            "4423a4303e1b4f1ca44eba00c2992245",
            "b32b18e9f1a442b09d2c2442d89d237f",
            "b498029831414af292b413072fb96d9f"
          ]
        },
        "id": "g8o5FJ6n9J7c",
        "outputId": "332fa642-3729-4c08-9499-fc5aa2a16e4a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30d9d666498f40038943b0eb69f38819"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.csv:   0%|          | 0.00/4.75M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92b9393af1d041929ab074ab02243d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.csv:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c91a2bd7ebf943dbaa682a5bc30dfbf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af81b3970d2e4a2c8dec6bbab10d1283"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4f30a9dbc0b4b62a137d4eb742c99af"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X6HrpprwIrIz",
        "outputId": "f1cec7cc-a1fc-4c6a-e5de-50eb602c72cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['text', 'ft_tense'],\n",
              "     num_rows: 51247\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['text', 'ft_tense'],\n",
              "     num_rows: 14642\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset[\"train\"], dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwADTfpJh8LY",
        "outputId": "9bc07043-89e0-45d2-cc0b-cdbf05e6fd85"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'And I understand, frankly, every President, for the last many Presidents, have said, Were going to open our embassy in Jerusalem. And then they never did it.',\n",
              " 'ft_tense': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEA1ju653l-p"
      },
      "source": [
        "## Causal Language modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-WGBCO343l-q"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilgpt2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "AitJ-qGvK2BI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iAYlS40Z3l-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "aee9a50e0a3743c59da9c27302685cdf",
            "31829959b6cf42b7a4bcaa0f5bae3576",
            "5dcea0e01f6748f3b62e2c4d974eac6b",
            "90a688f552a4445fbf33668e19f9fde9",
            "7ddce0b87f5a4ef7b5cbd627165774ff",
            "c2c645463bb94bd39507b4d9b6813c14",
            "39e28e9004454dd99ba7f4a57908c25f",
            "0e894528e9aa4ae489a7e8651a587c99",
            "e3338412ef874a75af924d65b12aa11f",
            "1dbe0106fbdf4595a7e260b5d4d28761",
            "38dd3854816d495b94b7bec30fe85ce3",
            "7191eaaa0adc4214ae4abd4ba1a9d53a",
            "cb26570588944ae98b228d1650f7779c",
            "1f6519572b50437eaf1e26621c8d8c7e",
            "04c4fe1035754472bfec746adad9c9de",
            "812dd67abaaa40d497df168522a016be",
            "ad645c91bb33472ca73f4a0dc1fc0846",
            "b4f88e4cf2fd451380d59c0f048bfebf",
            "05610699fbc64386a8880ad6f1bbf6cc",
            "d427188848c54955972f24eaa00272de",
            "33e84b3cd28d4a9093410ea37de158f3",
            "2d445cbb6de74c9ebf9821df54dd5903",
            "de82c5bc886a4431b185059d979ab261",
            "a7ce1f1d55704a658823f96325c4fe48",
            "e3a9c82fec3b49fdb06121449feb254a",
            "57d4df492bb746ee97dd72a29041f5c8",
            "744cc941440145078e14f3bb6e731f4e",
            "ac797edbdbb7496e9f3c636df0cf54cd",
            "0837629154f5481fa95d66c25f6bcef9",
            "a401ef5a83c74a79bdd4f56111ee4049",
            "fb89ba02600341fea2f904d5bc88fc6d",
            "e1420fe1acce4f04954c74a2d7fe8a6e",
            "9c403e1d987b4d94a9f237f25dac90d6",
            "0d880b530e8344e9b53fea9a4f724c63",
            "c8e0df45f79d439ba4f0315810d2195d",
            "aaaa130721e24f679b27efcdc3b75c60",
            "384a92be406a448e9a5199341ad28bdc",
            "e87570a8d56e42f9b96c80f29d50ea08",
            "6cb3793a0eab4cb2bfef5935257b203a",
            "f5e62a6867344611b90b699994437029",
            "a02a4ddef3a243bf8fe87cbf2748853f",
            "e1699aa77de0405f84c703dc92bd631c",
            "af3cb867c58145fdbd53d29377b3f5ae",
            "ada58df4f2604cfa9b503fb65c106780",
            "f4672ef24bec45449936307030eff3dd",
            "8af201c5b85e42b6b2269a66284587e2",
            "b1a02066fe2e4dfdb0cebdb628f182ae",
            "f478de08020540eabcb1d90c8e93f8c1",
            "6c77471fa731491bb3f5b039e25e4ede",
            "a03ccaa0ac1442a6a1c9109453e2d2f5",
            "102ef071835141248b5c28c85a73883b",
            "f503fda792a242c5ba4cec5c61c22470",
            "f8cf8d2cdeb44ff8989ee51bd7f3cf8a",
            "e08ae57aceb24ab7abcdf76b14c41a1f",
            "eed0a13ed882496284030c27b5fa84be"
          ]
        },
        "outputId": "cda5dd48-06bf-4283-a89a-e5adbfc56ae8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aee9a50e0a3743c59da9c27302685cdf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7191eaaa0adc4214ae4abd4ba1a9d53a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de82c5bc886a4431b185059d979ab261"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d880b530e8344e9b53fea9a4f724c63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4672ef24bec45449936307030eff3dd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lS2m25YM3l-z"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NVAO0H8u3l-3",
        "outputId": "8e869c35-f273-49f8-c36c-a2da93046cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "1323a6a34f004aadac9f45816c8eb420",
            "2d6134a137744677a6d8470c52a9c3e2",
            "6054a4adf31c45a697f3ff4770492ec1",
            "3cd09df8d50e4a0fbd42bcc19d58d54d",
            "8d2671668d7d4093877a1ce0c71d4a3b",
            "6e22b2f4757241f2812f7af1897c3f54",
            "52c03c8cc99f433f8cd2f8e696ab29b1",
            "9bf71e498f104bcda52281364db12131",
            "76d5491476f642d4b3110397c57f7b54",
            "82994301fb83433c8e303254ce60b74b",
            "443fe75985214c14afd1cfecc2f438da",
            "61d6bb0b3d384ff88cd16dbcce1afea2",
            "b68fcb565a2643e4850ea1e531ac485a",
            "6ddc3a718bcd4f479c813a717e1f7600",
            "41eca919442b46a2ac92453fb9396fcc",
            "c6982d4cc8554802ba24c3b2e6964b36",
            "a37d3da8b5f64154b4cbef6b516c4d39",
            "77dafdd8468048b58caccde18306474e",
            "c93836826f6149729f0fc1454f1c5686",
            "b58b02c71fd44ad8b6f95e9ae563401b",
            "c9f30fc0f671437f814193e2fcd446d5",
            "3e088a0280fe44efaeeea05f8cad20e0"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/51247 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1323a6a34f004aadac9f45816c8eb420"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/14642 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61d6bb0b3d384ff88cd16dbcce1afea2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=[\"text\", \"ft_tense\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvh4rswz-fP6",
        "outputId": "e9d1fade-9204-455d-e928-62f5d4aea37b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'And I understand, frankly, every President, for the last many Presidents, have said, Were going to open our embassy in Jerusalem. And then they never did it.',\n",
              " 'ft_tense': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nYv_mcKk3l-7",
        "outputId": "f23bf676-e20b-489d-dfe1-1f50e496afe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [1870,\n",
              "  314,\n",
              "  1833,\n",
              "  11,\n",
              "  17813,\n",
              "  11,\n",
              "  790,\n",
              "  1992,\n",
              "  11,\n",
              "  329,\n",
              "  262,\n",
              "  938,\n",
              "  867,\n",
              "  35506,\n",
              "  11,\n",
              "  423,\n",
              "  531,\n",
              "  11,\n",
              "  15176,\n",
              "  1016,\n",
              "  284,\n",
              "  1280,\n",
              "  674,\n",
              "  18613,\n",
              "  287,\n",
              "  10843,\n",
              "  13,\n",
              "  843,\n",
              "  788,\n",
              "  484,\n",
              "  1239,\n",
              "  750,\n",
              "  340,\n",
              "  13],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenized_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppc1EV0J6VH7",
        "outputId": "df639aa8-9789-4351-cd9a-b4a041f0c13d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DVHs5aCA3l-_"
      },
      "outputs": [],
      "source": [
        "block_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iaAJy5Hu3l_B"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gXUSfBrq3l_C",
        "outputId": "adb3a4c3-292e-4587-8fff-b3f7654db5ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e5c3ee0f474142cd8bad118f0584148c",
            "ef4f180ddeec4cb9bd2240111ed9cdf8",
            "e86a50f323d349048d11f2034e24c059",
            "d975d081bcb04aa6a1fa684f62769b6a",
            "c802bbdf3fbb46098000e9d3559536ea",
            "592cd3b21bed4415a9fc73be4cc62deb",
            "09b7e2cad9ed40eeb342bdaf2a414741",
            "3468af9be411492c9dbd23472611917f",
            "4a2f3e1cdb824e1daac45fc10a2a2687",
            "f4ca31af232f42e0b201c56a201c7af0",
            "54883b61645948dbba5fdf3d09525f06",
            "2a428034c73341c58b39fc9026e627cc",
            "4a89effdb77d4c258899b64482e76ba8",
            "ac91eb4b37d54e6bb8f59721704ebd4a",
            "f94eeb48927e4db491dcd0d86a7d1b67",
            "e8591afd902f43b8960f05afdd6e1829",
            "a7ed5afff3ec4a79a3bfd1669dd30bca",
            "e5f05236a1a4489cb8f54ae80b22b193",
            "75b8ffd63fd2427fa70ec3e84b9fee40",
            "46a0ac5b6187428ba747ca26640ce5a6",
            "713097d6eab24e90ad50687ac2b446b0",
            "cdfc41143b224b55adb021807f91910c"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/51247 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5c3ee0f474142cd8bad118f0584148c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/14642 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a428034c73341c58b39fc9026e627cc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "lm_dataset = tokenized_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hTeGCLl_3l_G",
        "outputId": "cc4e85f8-0d28-4a65-ccdb-0d37c7f3038c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'And I understand, frankly, every President, for the last many Presidents, have said, Were going to open our embassy in Jerusalem. And then they never did it.And youll be glad to know the United States will continue to be a driver of global growth.Were going to have a great red wave.And I knew I was going to be here with you to mark this momentous occasion, so I wore my Houston Astros tie because theyre still the champion of the World Series.And whenever youre dealing with the medical field, its its a serious thing.And the test result comes back in five minutes, and we have great testing.Opportunity Scholarship Program right here in the district.And many of you, I hope, will be following me.So, were so happy for your family, and I know your sons are going to make you very proud.And at this time, our team is working with the governor and working with the senator to ensure that we flow testing resources.All over the world.There will be some areas hit harder than we think.And well be announcing, over the next very short period of time, exactly what were going to be doing.And they cant do what they were saying theyre going to do because if they'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tokenizer.decode(lm_dataset[\"train\"][0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_dataset[\"test\"][0][\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "LM6OEyV4nCIb",
        "outputId": "b6f1e0f7-defb-425f-b22f-afe54c936847"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'And because of that, a repeat colonoscopy was not indicated at this time and will be deferred until his next periodic physical exam.And we are going to realize a free and open Indo-Pacific economic growth.And we are going to be signing a full and complete pardon.I dont think youre going to stop it at all.Were going to start drilling in ANWR one of the largest oil reserves in the world that for 40 years this country was unable to touch.MR. SPICER: I will ask, Alexis.And I think that he has been very remarkably adept at fielding our inquiries, and I know hes going to go back and help to continue this effort.And I guess were going to be a little bit late.A lot of people are going to be evicted, but Im going to stop it because Ill do it myself if I have to.He quickly encountered an insurgent who was about to fire a rocket-propelled grenade at his squad.hiring rose more than forecast in May, wages picked up and the unemployment rate matched the lowest in almost five decades, indicating the strong labor market will keep powering economic growth.MS. SANDERS: I think thats something that those senators will have to answer to their constituents.Two of our'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "TnJCgLRfK-K8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sPqQA3TT3l_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "0c6c636838504745bd90b54c9861510f",
            "0d0631bf5c554451b56802e09e8548fd",
            "e965e22370bf465ea05889c3cfeb954f",
            "e6b59bbaedff42b18b4f7fd2bcaef919",
            "15af11171fa742a9b0cfc03180b84a9e",
            "9b07be3fe8fa4610a91cc2206bba0c18",
            "6b24fa11624248c79cf79a21293ae01a",
            "d3febf96dc5449cca01cef76bafa0872",
            "9ce87bd54ec34f93ab86b0be0e1f84a4",
            "5319cd7f78e14e44a614aa59f736fb84",
            "75f025d9e5a648e18d8dc1c1982b5ea8",
            "56ac7a5970f94c649f43ad4288da8cf9",
            "fc1b6cbaa07444298e1b946801141d67",
            "ef2a3ec78f2b48f6bd9d5f453b378210",
            "785ae136045044a9870964773f9c110a",
            "ecb7592762794a1a9b4ae1498dfbf051",
            "b355651baf9c425eaca92e86d53dc31a",
            "b230f7a90b2a47399c1d8f442497812a",
            "f431e453692c43ec8e6baf4ba1bf6a62",
            "9b295e9a19d8493787e2ab4cb719c603",
            "abbe7a941f154021a359cde59e61af17",
            "63fca1875ded4cb9b141b70fd9a379e1"
          ]
        },
        "outputId": "5ee3b27f-d98d-41b4-e576-548e27fd3986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c6c636838504745bd90b54c9861510f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ac7a5970f94c649f43ad4288da8cf9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable} / {total} ({100 * trainable / total:.2f}%)\")"
      ],
      "metadata": {
        "id": "axb7M1X1IBaN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_PTxNiSG4M-",
        "outputId": "22e37336-9785-4c14-9014-27274a3cc135"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 81912576 / 81912576 (100.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jElf8LJ33l_K"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YbSwEhQ63l_L"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    model_checkpoint,\n",
        "    eval_strategy = \"epoch\",\n",
        "    label_names=[\"labels\"],\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\", # disable wandb\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OEuqwIra3l_N"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset[\"train\"],\n",
        "    eval_dataset=lm_dataset[\"test\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NyZvu_MF3l_P",
        "outputId": "f354aeaa-ba03-4d43-907a-1632531a105b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1428/1428 09:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.377837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.534400</td>\n",
              "      <td>3.337214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.375400</td>\n",
              "      <td>3.327589</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1428, training_loss=3.4177546995360646, metrics={'train_runtime': 564.4346, 'train_samples_per_second': 20.208, 'train_steps_per_second': 2.53, 'total_flos': 745087684313088.0, 'train_loss': 3.4177546995360646, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "train_output = trainer.train()\n",
        "train_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_output.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW3B5E0XLMX6",
        "outputId": "b8f22331-4ca2-4cfe-fe5a-49e08eb8db98"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_runtime': 564.4346,\n",
              " 'train_samples_per_second': 20.208,\n",
              " 'train_steps_per_second': 2.53,\n",
              " 'total_flos': 745087684313088.0,\n",
              " 'train_loss': 3.4177546995360646,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "G8Bi4-HoJoCj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "diKZnB1I3l_R",
        "outputId": "6462d2fd-da08-4648-e7fd-6cc0db62affd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [135/135 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.327589273452759,\n",
              " 'eval_runtime': 14.3086,\n",
              " 'eval_samples_per_second': 75.479,\n",
              " 'eval_steps_per_second': 9.435,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "eval_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy_loss = eval_results['eval_loss']\n",
        "print(f\"Cross Entropy Loss:{cross_entropy_loss:.4f} Perplexity:{math.exp(cross_entropy_loss):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiawONWvFf4F",
        "outputId": "322ab460-0dc4-45cb-eedd-bdce9eaf51e5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy Loss:3.3276 Perplexity:27.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "7_0E87gtKI11"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8tEx6BXCRNw",
        "outputId": "6895eef1-b7b6-4cd4-ce49-73af1b5dafd0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Donlad Trump says China\""
      ],
      "metadata": {
        "id": "8DMxgP129qJQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "  out = generator(prompt, pad_token_id=generator.tokenizer.eos_token_id)\n",
        "  print(out[0]['generated_text'])"
      ],
      "metadata": {
        "id": "iet2adklCQ1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80aa2724-bf3c-4c7f-b33c-bab528604c40"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Donlad Trump says China isnt a friend.We have a great job that people dont like, but we have tremendous businesses going through this area.But they didnt like it, and theyre going to get it a little bit lower than whats happening\n",
            "Donlad Trump says China could be a bigger problem.But I will say: I will try to go ahead and say, How will he even do it if?We had these guys all living in an area and you all know that youllnt\n",
            "Donlad Trump says China has its problems, but China will fix them because thats not right.And were going to have all of these issues solved, and I will not compromise on them.On Tuesday, President Xi will meet with other countries for a\n",
            "Donlad Trump says China will help build the Americas.And he will not just say those are the problems in China.MR. SPICER: Yeah.They wont, I will.Theres no longer the problem as to who will become the\n",
            "Donlad Trump says China will never have to deal with ISIS, and we have not even known or saw them before.And he thinks so, and he knows what she hes going to tell him about it.MR. SPICER: Sorry,\n",
            "Donlad Trump says China will no longer pay for our exports but its done.And I mean, whats going on this, were you going to get that on the table?And Im also going to get you going to see more cases in Michigan.\n",
            "Donlad Trump says China will destroy Israel, including on nuclear arms.I will tell you, the President is going to be listening to you very closely.And we will never stop fighting alongside and against terrorists.Its hard to even get them to do\n",
            "Donlad Trump says China will be our partner.In a sense, it would mean good things for the American people, our companies and our businesses, but were going to have to go to Cuba, and I think thats the point.And the other\n",
            "Donlad Trump says China will be very worried.But with the President going down, and he wants to go back on his rhetoric.And I know that a lot of their money is going to be going toward the hospital.With the help of the\n",
            "Donlad Trump says China will come back for trade deals with us.And the Democrats will be happy, too.MR. SPICER: Theyre going to tell you, but I promise you you, I think the Democrats will actually get this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning with LoRA"
      ],
      "metadata": {
        "id": "UtRxyI_bJJW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_path_starts_with()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOmzZmHp8ygF",
        "outputId": "e6acc326-c54d-4964-8315-2a31f8802376"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted folder: distilgpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType"
      ],
      "metadata": {
        "id": "eDU2hvOlCQwG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\"],  # Specific to GPT2's attention\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")"
      ],
      "metadata": {
        "id": "uh_X00AbJgUd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "TSRn2bb4L9hn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = get_peft_model(base_model, lora_config)"
      ],
      "metadata": {
        "id": "hic4K_f5CQrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567a7614-2fbc-45ff-9643-72876b3a401a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "GZLsb_rgCQmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6431ce56-52c1-4e2b-e634-b75a5278f330"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 147,456 || all params: 82,060,032 || trainable%: 0.1797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    model_checkpoint,\n",
        "    eval_strategy = \"epoch\",\n",
        "    label_names=[\"labels\"],\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\", # disable wandb\n",
        ")"
      ],
      "metadata": {
        "id": "V-Y2gluGwoUZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset[\"train\"],\n",
        "    eval_dataset=lm_dataset[\"test\"],\n",
        ")"
      ],
      "metadata": {
        "id": "benOFDS5J2v_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_train_output = lora_trainer.train()\n",
        "lora_train_output"
      ],
      "metadata": {
        "id": "125fHsgHCQhi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "87ad403a-4bd4-4f2d-93da-be34fe36d61d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1428/1428 06:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.805296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.955800</td>\n",
              "      <td>3.732754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.828800</td>\n",
              "      <td>3.714524</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1428, training_loss=3.861566367269564, metrics={'train_runtime': 374.7405, 'train_samples_per_second': 30.437, 'train_steps_per_second': 3.811, 'total_flos': 747671056809984.0, 'train_loss': 3.861566367269564, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_eval_results = lora_trainer.evaluate()\n",
        "lora_eval_results"
      ],
      "metadata": {
        "id": "tROtB6EkCQcF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "70de9fc2-cd6a-4b61-afea-1924077e999c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [135/135 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.714524030685425,\n",
              " 'eval_runtime': 14.5183,\n",
              " 'eval_samples_per_second': 74.389,\n",
              " 'eval_steps_per_second': 9.299,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_cross_entropy_loss = lora_eval_results['eval_loss']\n",
        "print(f\"Cross Entropy Loss:{lora_cross_entropy_loss:.4f} Perplexity:{math.exp(lora_cross_entropy_loss):.2f}\")"
      ],
      "metadata": {
        "id": "b_iBbXV8CQXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e38149-c0cc-4507-c025-c485d3d00763"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy Loss:3.7145 Perplexity:41.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model_checkpoint = f\"lora_{model_checkpoint}\"\n",
        "lora_model_checkpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ww21jKbkxPYW",
        "outputId": "d3676f9f-f0f2-4e2a-a63e-1f870bc10324"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lora_distilgpt2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.save_pretrained(lora_model_checkpoint)\n",
        "tokenizer.save_pretrained(lora_model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwwAgl8TxC-u",
        "outputId": "985c192c-286e-4a26-ecd3-0d504cf88801"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_distilgpt2/tokenizer_config.json',\n",
              " 'lora_distilgpt2/special_tokens_map.json',\n",
              " 'lora_distilgpt2/vocab.json',\n",
              " 'lora_distilgpt2/merges.txt',\n",
              " 'lora_distilgpt2/added_tokens.json',\n",
              " 'lora_distilgpt2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "X8Kc_YwRCQSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "347085a3-3f7a-41f0-8f3c-781fa10b4bd1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "EmDfkY9vvPrn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_generator.model = PeftModel.from_pretrained(\n",
        "    model=lora_model,\n",
        "    model_id=lora_model_checkpoint\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeiOQPyvxm7g",
        "outputId": "89dce857-85ff-4c72-b406-d9b1e3022254"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight']\n",
            "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "  out = lora_generator(prompt, pad_token_id=generator.tokenizer.eos_token_id)\n",
        "  print(out[0]['generated_text'])"
      ],
      "metadata": {
        "id": "BNvgzMoDCQNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a86ccd-805e-4b88-d9d6-86317f114939"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Donlad Trump says China wants an apology, but Chinese media reports suggest the presidents response is likely too low.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donlad Trump says China will cut off trade with America and \"start a new global manufacturing factory on our doorstep\" in 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donlad Trump says China could develop new nuclear weapons in exchange for nuclear weapons Read more\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donlad Trump says China will not pay for arms for its air force if it doesn't get military support\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donlad Trump says China wouldn't be able to \"buy\" the United States because of a trade deal between China and the US, the Times of Taiwan reported on Sunday.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donlad Trump says China is obsolete \n",
            "\n",
            "\n",
            "In a column by Dr Alex Jones, the top Democrat on CNN's Morning Joe, Jones claimed that Chinese economic growth is obsolete \n",
            "Donlad Trump says China will cut jobs and cut taxes for the middle class\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Trump told an audience Tuesday in Melbourne, Australia, his business group the Wall Street Journal reported that as American investors tried to buy the New\n",
            "Donlad Trump says China is working hard to reduce human disease worldwide for three quarters of their projected $100bn.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donlad Trump says China will 'go away' after Trump says he believes Russia interfered in the 2016 presidential election  and that he believes Russia interfered in the 2016 presidential election\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donlad Trump says China is the greatest single buyer, according to a new Wall Street Journal survey. That was reported last month, as a report was published last month by the Federal Reserve. He says the country's largest real estate industry will continue to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show datasets transformers peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRYITfcP6-Xn",
        "outputId": "a894ada4-88c1-461c-f055-986c2ff54c91"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: datasets\n",
            "Version: 3.5.0\n",
            "Summary: HuggingFace community-driven open-source library of datasets\n",
            "Home-page: https://github.com/huggingface/datasets\n",
            "Author: HuggingFace Inc.\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
            "Required-by: \n",
            "---\n",
            "Name: transformers\n",
            "Version: 4.51.3\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n",
            "---\n",
            "Name: peft\n",
            "Version: 0.14.0\n",
            "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
            "Home-page: https://github.com/huggingface/peft\n",
            "Author: The HuggingFace team\n",
            "Author-email: benjamin@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: accelerate, huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGllUGhD_nOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
