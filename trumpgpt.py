# -*- coding: utf-8 -*-
"""TrumpGPT

Automatically generated by Colab.
"""
import math
import os
import shutil

from datasets import load_dataset
from peft import get_peft_model, LoraConfig, PeftModel, TaskType
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments


def remove_path_starts_with(starts_with=["lora", "distilgpt", "sample_data"]):
    for entry in os.listdir():
        if os.path.isdir(entry) and any(entry.startswith(prefix) for prefix in starts_with):
            shutil.rmtree(entry, ignore_errors=True)
            print(f"Deleted folder: {entry}")

remove_path_starts_with()


# Preparing the dataset
data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset('jonaskoenig/trump_administration_statement', data_files=data_files)

dataset["train"], dataset["test"]

dataset["train"][0]

# Causal Language modeling
model_checkpoint = "distilgpt2"

tokenizer = AutoTokenizer.from_pretrained(
    model_checkpoint,
    trust_remote_code=True,
    use_fast=True
)

def tokenize_function(examples):
    return tokenizer(examples["text"])

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=4,
    remove_columns=["text", "ft_tense"]
)

block_size = 256

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
    # customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_dataset = tokenized_dataset.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)

model = AutoModelForCausalLM.from_pretrained(model_checkpoint)

def print_trainable_parameters(model):
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"Trainable parameters: {trainable} / {total} ({100 * trainable / total:.2f}%)")

print_trainable_parameters(model)

training_args = TrainingArguments(
    model_checkpoint,
    eval_strategy = "epoch",
    label_names=["labels"],
    learning_rate=2e-5,
    weight_decay=0.01,
    push_to_hub=False,
    report_to="none", # disable wandb
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
)

train_output = trainer.train()
print(train_output)

eval_results = trainer.evaluate()
print(eval_results)

cross_entropy_loss = eval_results['eval_loss']
print(f"Cross Entropy Loss:{cross_entropy_loss:.4f} Perplexity:{math.exp(cross_entropy_loss):.2f}")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
prompt = "Donlad Trump says China"

for _ in range(10):
  out = generator(prompt, pad_token_id=generator.tokenizer.eos_token_id)
  print(out[0]['generated_text'])

# Fine-Tuning with LoRA

remove_path_starts_with()

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["c_attn"],  # Specific to GPT2's attention
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)

base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)
lora_model = get_peft_model(base_model, lora_config)

print(lora_model.print_trainable_parameters())

training_args = TrainingArguments(
    model_checkpoint,
    eval_strategy = "epoch",
    label_names=["labels"],
    learning_rate=2e-5,
    weight_decay=0.01,
    push_to_hub=False,
    report_to="none", # disable wandb
)

lora_trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
)

lora_train_output = lora_trainer.train()
print(lora_train_output)

lora_eval_results = lora_trainer.evaluate()
print(lora_eval_results)

lora_cross_entropy_loss = lora_eval_results['eval_loss']
print(f"Cross Entropy Loss:{lora_cross_entropy_loss:.4f} Perplexity:{math.exp(lora_cross_entropy_loss):.2f}")

lora_model_checkpoint = f"lora_{model_checkpoint}"
lora_model.save_pretrained(lora_model_checkpoint)
tokenizer.save_pretrained(lora_model_checkpoint)

lora_generator = pipeline(
    "text-generation",
    model=base_model,
    tokenizer=tokenizer
)

lora_generator.model = PeftModel.from_pretrained(
    model=lora_model,
    model_id=lora_model_checkpoint
)

for _ in range(10):
  out = lora_generator(prompt, pad_token_id=generator.tokenizer.eos_token_id)
  print(out[0]['generated_text'])
